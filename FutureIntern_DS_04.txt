import pandas as pd
import numpy as np
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.pipeline import Pipeline
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

# Download necessary NLTK data
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Use raw strings to avoid issues with backslashes for file paths
csv_file_path = r'/content/spam_assassin.csv'

# Function to load email data from the CSV file
def load_email_data_from_csv(csv_path):
    try:
        df = pd.read_csv(csv_path)
        # Check if the required columns 'text' and 'target' exist
        if 'text' not in df.columns or 'target' not in df.columns:
            raise KeyError("Missing 'text' or 'target' column in the CSV file.")

        texts = df['text'].tolist()  # Extract email texts
        labels = df['target'].tolist()  # Extract corresponding labels (0 for ham, 1 for spam)
        return texts, labels
    except FileNotFoundError:
        print(f"File not found at the path: {csv_path}")
        return [], []
    except KeyError as e:
        print(f"Column not found: {e}")
        print(f"Available columns are: {df.columns}")
        return [], []
    except pd.errors.EmptyDataError:
        print(f"CSV file is empty or has no data.")
        return [], []
    except Exception as e:
        print(f"An error occurred: {e}")
        return [], []

# Load emails
texts, labels = load_email_data_from_csv(csv_file_path)

# Check if data was loaded properly
if not texts or not labels:
    print("Failed to load data. Please check the file and column names.")
else:
    # Create DataFrame with email text and corresponding labels
    df = pd.DataFrame({
        'text': texts,
        'label': labels
    })

    # Print a sample of the data for inspection
    print(df.sample(5))

    # Text Preprocessing (Tokenization, Lemmatization, Cleaning)
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    def clean_email_text(text):
        # Convert text to lowercase
        text = text.lower()

        # Remove any URLs, emails, or special characters
        text = re.sub(r'http\S+|www\S+|@\S+', '', text)
        text = re.sub(r'[^a-z\s]', '', text)  # Retain only letters and whitespace

        # Tokenize the text
        tokens = word_tokenize(text)

        # Remove stopwords and apply lemmatization
        cleaned_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]

        return ' '.join(cleaned_tokens)

    # Apply the cleaning function to the text data
    df['cleaned_text'] = df['text'].apply(clean_email_text)

    # Split the dataset into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(df['cleaned_text'], df['label'], test_size=0.25, random_state=42)

    # Build a pipeline for text feature extraction and classification using SVM
    model_pipeline = Pipeline([
        ('vectorizer', CountVectorizer(max_features=4000)),  # Use CountVectorizer for text features
        ('svm', SVC(kernel='linear', random_state=42))       # Linear SVM for classification
    ])

    # Train the model using the training data
    model_pipeline.fit(X_train, y_train)

    # Cross-validation on training data
    cross_val_scores = cross_val_score(model_pipeline, X_train, y_train, cv=5)
    print(f"Cross-validation scores: {cross_val_scores}")
    print(f"Mean cross-validation accuracy: {np.mean(cross_val_scores):.4f}")

    # Predictions on the test set
    y_pred = model_pipeline.predict(X_test)

    # Evaluate the model performance
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Print evaluation results
    print("Evaluation Results on Test Data:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1 Score: {f1:.4f}")

    # Print detailed classification report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))